{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mido import MidiFile, tick2second\n",
    "from pretty_midi import PrettyMIDI\n",
    "import pickle\n",
    "import joblib\n",
    "import subprocess\n",
    "import os\n",
    "import os.path\n",
    "import random\n",
    "from ExtractBootlegFeatures1 import *\n",
    "import multiprocessing\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Bootleg Features from Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_Pdf_wrapper(i):\n",
    "    basedir = 'experiments/allBootlegScores-new/'\n",
    "    outdir = '{}/scores/'.format(basedir)\n",
    "    imagedir = '/data1/dyang/Datasets/5000dataset/images1/'\n",
    "    numPdfs=5024;\n",
    "    if i >= numPdfs:\n",
    "        return\n",
    "    if not os.path.isdir(outdir):\n",
    "        os.makedirs(outdir)\n",
    "    \n",
    "    outfile = outdir + 'p{}.pkl'.format(str(i))\n",
    "    if i <= 200:\n",
    "        filepath = imagedir + 'p'+str(i)+'/'\n",
    "    else:\n",
    "        filepath = imagedir + 'p'+str(i)+'/'\n",
    "    if os.path.exists(outfile):\n",
    "        print(str(i)+\" is already there\")\n",
    "        return\n",
    "    total_bscore = np.zeros((62,1))\n",
    "    lst = 1 #len(os.listdir(filepath))\n",
    "    if lst == 1:\n",
    "        if i<=200:\n",
    "            imagepath = '20210726165646.jpg' #'p{}.jpg'.format(i)\n",
    "        else:\n",
    "            imagepath = filepath+'p{}.jpg'.format(i)\n",
    "        try:\n",
    "            bscore_query = processImageFile(imagepath,None)\n",
    "        except:\n",
    "            bscore_query = np.zeros((62,1))\n",
    "        total_bscore = np.concatenate((total_bscore, bscore_query),axis=1)\n",
    "    else:\n",
    "        for count in range(lst):\n",
    "            try:\n",
    "                if i<=200:\n",
    "                    imagepath = filepath+'p{}-{}.jpg'.format(i,count)\n",
    "                else:\n",
    "                    imagepath = filepath+'p{}-{}.jpg'.format(i,count)\n",
    "                bscore_query=processImageFile(imagepath,None)\n",
    "            except:\n",
    "                bscore_query = np.zeros((62,1))\n",
    "            total_bscore = np.concatenate((total_bscore, bscore_query),axis=1)\n",
    "    with open(outfile, 'wb') as f:\n",
    "        print(\"Saving p\" + str(i))\n",
    "        pickle.dump(total_bscore, f)\n",
    "    print(total_bscore.shape)\n",
    "        #visualizeLongBootlegScore(total_bscore, [13,15,17,19,21,35,37,39,41,43])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23 is already there\n"
     ]
    }
   ],
   "source": [
    "convert_Pdf_wrapper(23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inputs = [[i] for i in range(1,4825)]\n",
    "n_cores = 28\n",
    "\n",
    "pool = multiprocessing.Pool(processes=n_cores)\n",
    "outputs = list(pool.starmap(convert_Pdf_wrapper, inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct Reverse Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "piece_to_num = {}\n",
    "piecedir = \"cfg_files/db.list\"\n",
    "with open(piecedir, \"r\")as f:\n",
    "    for i, piece in enumerate(f):\n",
    "        piece = piece.strip().strip('\\n')\n",
    "        piece =piece.split('/')[-1][:-4]\n",
    "        piece_to_num[i] = piece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"piece_to_num.pkl\",'wb')as f:\n",
    "    pickle.dump(piece_to_num,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def bootlegHash(arr):\n",
    "    bitstring = \"\"\n",
    "    for i in range(len(arr)):\n",
    "        if arr[i]==1:\n",
    "            bitstring+=\"1\"\n",
    "        else:\n",
    "            bitstring +=\"0\"\n",
    "    bitstring = bitstring+\"00\"\n",
    "    hashint = int(bitstring, 2)\n",
    "    hashint = np.uint64(hashint)\n",
    "    return hashint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def getTotalBscore(bscore_file):\n",
    "    bscore_array = []\n",
    "    with open (bscore_file,'rb') as f:\n",
    "        bscore_array = pickle.load(f)\n",
    "    f.close()\n",
    "    total_bscore = np.array([]).reshape(62,0)\n",
    "    page_array = []\n",
    "    for page in bscore_array:\n",
    "        total_page = np.array([]).reshape(62,0)\n",
    "        for num in page:\n",
    "            col = np.array(decodeColumn(num)).reshape(62,-1)\n",
    "            total_page = np.concatenate((total_page,col),axis=1)\n",
    "        total_bscore = np.concatenate((total_bscore,total_page),axis=1)\n",
    "        page_array.append(total_page)\n",
    "    return total_bscore,page_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decodeColumn(num):\n",
    "    col = []\n",
    "    for i in range(62):\n",
    "        col.insert(0,num%2)\n",
    "        num = int(num/2)\n",
    "    return col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def getRandomdb(size, mode, seed = 1):\n",
    "    # Generates db.list\n",
    "    f = open(\"cfg_files/db.list\", \"w\")\n",
    "    random.seed(seed)\n",
    "    randomDir = []\n",
    "    if(mode == \"all\"):\n",
    "        directory = 'experiments/bootleg_output/'\n",
    "        for root, dirs, files in os.walk(directory):\n",
    "            for file in files:\n",
    "                fname = os.path.join(root,file)\n",
    "                if fname[-4:] == \".pkl\":\n",
    "                    num = fname.split('/')[-1]\n",
    "                    str2 = 'experiments/bootleg_output/'+num+\"\\n\"\n",
    "                    f.write(str2)\n",
    "        f.close()    \n",
    "        return\n",
    "    if size > 200:\n",
    "        randomDir = random.sample(range(201, 36834), size-200)\n",
    "    print(randomDir)\n",
    "    for i in range(1,201):\n",
    "        num = str(i)\n",
    "        str2 = 'experiments/bootleg_output/'+\"p\"+num+\".pkl\"+\"\\n\"\n",
    "        f.write(str2)\n",
    "        \n",
    "    for i in randomDir:\n",
    "        num = str(i)\n",
    "        str2 = 'experiments/bootleg_output/'+\"p\"+num+\".pkl\"+\"\\n\"\n",
    "        f.write(str2)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "getRandomdb(200,\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Singular_DB(data, rindex):\n",
    "    for colindex in range(len(data.T)):\n",
    "        col = data.T[colindex]\n",
    "        hashint = bootlegHash(col)\n",
    "        if hashint == 0:\n",
    "            continue\n",
    "        pieceStr = curfile.split('/')[-1][:-4]\n",
    "\n",
    "        if hashint in rindex:\n",
    "            if pieceStr in rindex[hashint]:\n",
    "                rindex[hashint][pieceStr].append(colindex)\n",
    "            else:\n",
    "                rindex[hashint][pieceStr]=[colindex]\n",
    "        else:\n",
    "            rindex[hashint]={}\n",
    "            rindex[hashint][pieceStr]=[colindex]\n",
    "    return rindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def N_Gram_DB(data, rindex, num, N_Gram = 3):\n",
    "    for colindex in range(len(data)):\n",
    "        cols = []\n",
    "        try:\n",
    "            for i in range(N_Gram):\n",
    "                cols.append(data[colindex+i])\n",
    "        except IndexError:\n",
    "            continue\n",
    "        fp = []\n",
    "        equals_Zero = True\n",
    "        for column in cols:\n",
    "            hashint = bootlegHash(column)\n",
    "            fp.append(hashint)\n",
    "            if hashint != 0:\n",
    "                equals_Zero = False\n",
    "        if equals_Zero == True:\n",
    "            continue\n",
    "        fp = tuple(fp)\n",
    "        if fp in rindex:\n",
    "            if num in rindex[fp]:\n",
    "                rindex[fp][num].append(colindex)\n",
    "            else:\n",
    "                rindex[fp][num]=[colindex]\n",
    "        else:\n",
    "            rindex[fp]={}\n",
    "            rindex[fp][num]=[colindex]\n",
    "    return rindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Dynamic_N_Gram_DB(data, rindex, counts, num, threshold):\n",
    "    for colindex in range(len(data.T)):\n",
    "        N_Gram = 1\n",
    "        cols = []\n",
    "        while(True):\n",
    "            try:\n",
    "                hashint = bootlegHash(data.T[colindex+N_Gram-1])\n",
    "                if hashint == 0:\n",
    "                    break\n",
    "                cols.append(hashint)\n",
    "            except IndexError:\n",
    "                break\n",
    "            fp = tuple(cols)\n",
    "            numMatches = counts[N_Gram-1][fp]\n",
    "            if numMatches < threshold or N_Gram == 4:\n",
    "                if fp in rindex:\n",
    "                    if num in rindex[fp]:\n",
    "                        rindex[fp][num].append(colindex)\n",
    "                    else:\n",
    "                        rindex[fp][num] = [colindex]\n",
    "                else:\n",
    "                    rindex[fp] = {}\n",
    "                    rindex[fp][num] = [colindex]\n",
    "                break\n",
    "            N_Gram+=1\n",
    "    return rindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createCountFile(outfile, rindex):\n",
    "    rindex_count = {}\n",
    "    for key in rindex:\n",
    "        count = 0\n",
    "        subDict = rindex[key]\n",
    "        for key1 in subDict:\n",
    "            count += len(subDict[key1])\n",
    "        rindex_count[key] = count\n",
    "    with open (outfile,\"wb\") as f:\n",
    "        pickle.dump(rindex_count,f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "start = time.process_time()\n",
    "rindex = {}\n",
    "fpMap = {}\n",
    "filelist = 'cfg_files\\\\db.list'\n",
    "N=1\n",
    "outfile = 'G:\\\\experiments\\\\indices\\\\N_Gram_{}_ALL.pkl'.format(N)\n",
    "mode = \"N_GRAM\"\n",
    "with open(filelist, 'r') as f:\n",
    "    failed = []\n",
    "    for i, curfile in enumerate(f):\n",
    "        curfile = curfile.strip().strip('\\n')\n",
    "        #print(\"Processed:\", count)\n",
    "        try:\n",
    "            num = curfile.split('\\\\')[-1][0]\n",
    "            if(num == 'd'):\n",
    "                data, _ = getTotalBscore(curfile)  #这一步是将整数\n",
    "            else:\n",
    "                with open(curfile, 'rb') as pickle_file:\n",
    "                    data = pickle.load(pickle_file)\n",
    "            elif mode == \"N_GRAM\":\n",
    "                rindex = N_Gram_DB(data, rindex, i, N_Gram=N)   #固定长度长度\n",
    "            elif mode == \"DYNAMIC_N_GRAM\":\n",
    "                rindex = Dynamic_N_Gram_DB(data, rindex, counts, i, threshold)  #动态长度\n",
    "        except:\n",
    "            failed.append(curfile)\n",
    "    print(failed)\n",
    "with open(outfile,'wb') as f:\n",
    "    pickle.dump(rindex,f)\n",
    "if mode == \"N_GRAM\":\n",
    "    outfile = os.path.splitext(outfile)[0][:-3]+\"COUNT.pkl\"\n",
    "    createCountFile(outfile, rindex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构造db.list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "basepath = \"bootleg_total/\"\n",
    "filelist = 'cfg_files/db.list'\n",
    "with open(filelist,\"w\") as f:\n",
    "    for root,dirs,files in os.walk(basepath):\n",
    "        for filename in files:\n",
    "            f.write(os.path.join(root,filename)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "filelist = 'cfg_files/pb.list'\n",
    "with open(filelist,'w') as f:\n",
    "    for i in range(1,201):\n",
    "        f.write('bootleg_total/p'+str(i)+'.pkl\\n')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "498f09a89a5f886d852d1dec44fadc4103c980cd6aa618ce7b14c700a0543837"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 ('bootleg_score')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
